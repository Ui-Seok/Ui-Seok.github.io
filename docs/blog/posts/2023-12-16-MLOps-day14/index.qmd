---
title: "MLOps for MLE - 14"
description: Docker 를 활용한 API 서버 띄우기
author: "Ui Seok"
date: "2023-12-16"
categories: [mlops]
image: "https://picsum.photos/id/116/200/150"

page-layout: article
---

## Summary

1. Dockerfile 과 Docker Compose 파일 작성

2. API 서버 동작 확인

::: {.callout-note}
실습을 진행했던 코드를 보고싶으시다면 [여기](https://github.com/Ui-Seok/mlops-tutorial)를 눌러주세요
:::


## 실습

### 1. Dockerfile 작성

Model API 를 작동시킬 수 있는 API 서버의 Docker Image

``` dockerfile
FROM amd64/python:3.9-slim

WORKDIR /usr/app

RUN pip install -U pip &&\
    pip install mlflow==1.30.0 pandas scikit-learn "fastapi[all]"

COPY schemas.py schemas.py
COPY app.py app.py
COPY sk_model/ sk_model/

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--reload"]

```


### 2. Docker Compose

``` yaml
version: "3"

services:
  api-with-model:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: api-with-model
    ports:
      - 8000:8000
    healthcheck:
      test:
        - CMD
        - curl -X POST http://localhost:8000/predict
        - -H
        - "Content-Type: application/json"
        - -d
        - '{"sepal_length": 6.7, "sepal_width": 3.3, "petal_length": 5.7, "petal_width": 2.1}'
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  default:
    name: mlops-network
    external: true

```


### 3. API 서버 작동 확인

`http://localhost:8000/docs` 에 접속하여 Request Body 의 형태에 맞게 데이터를 전달해주면 Response Body 로 inference 결과를 확인할 수 있음

`curl` 을 이용하여 API 가 잘 작동하는지 확인하는 방법도 있음

![`curl` 로 정보를 전달하여 inference 결과를 확인할 수 있음](fig1.png)






## Reference

[ML Engineer를 위한 MLOps tutorial](https://mlops-for-mle.github.io/tutorial/)