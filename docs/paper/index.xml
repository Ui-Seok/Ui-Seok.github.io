<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Home</title>
<link>https://ui-seok.github.io/docs/paper/index.html</link>
<atom:link href="https://ui-seok.github.io/docs/paper/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Sat, 09 Nov 2024 15:00:00 GMT</lastBuildDate>
<item>
  <title>Learning to Prompt for Vision-Language Models</title>
  <dc:creator>Ui Seok</dc:creator>
  <link>https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/index.html</link>
  <description><![CDATA[ 



<p>Paper Link : <a href="https://arxiv.org/abs/2109.01134">Learning to Prompt for Vision-Language Models</a></p>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks.</p>
<p>And usually these models will perform downstream tasks via prompt.</p>
<p>However, since prompt engineering requires a lot of prior knowledge of the new domain to do well, and since small changes in wording can have a big impact on performance, a significant amount of time is spent tweaking the wording.</p>
<p>This paper proposes Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition.</p>
<p>Concretely, CoOp models a promptâ€™s context words with learnable vectors while the entire pre-trained parameters are kept fixed.</p>
<p>CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Co-Op</figcaption>
</figure>
</div>
<p>Traditional SOTA models in computer vision use discrete labels to perform supervised learning.</p>
<p>However, this learning paradigm has the limitation of not being able to make zero-shot predictions because it limits the performance of the model.</p>
<p>The vision-language model, represented by CLIP, is a promising alternative to the existing learning paradigm, but it suffers from the problem that the performance of the model changes significantly due to prompt engineering.</p>
<p>CoOp is a research project that aims to make prompt engineering, a non-trivial task, automatic and learning-based.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Model Architecture</figcaption>
</figure>
</div>
<p>A simple way to describe the CoOp is that itâ€™s about fine-tuning the prompt part of CLIP.</p>
<p>The overall CoOp learning process is well represented in the figure above.</p>
<p>We use CLIP as it is, but freeze all parameters, leave only the context of the prompt part as the learnable vector, and run GT and cross-entropy loss to learn.</p>
<p>The context is the most important part to note here, and CoOp has implemented two different contexts to cover different recognition tasks: one is unified context, where all classes have the same context, and the other is class-specific context, where each class has a different context.</p>
<p>The contributions in the paper are as follows:</p>
<ol type="1">
<li><p>We present a timely study on the adaptation of recently proposed vision-language models in downstream applications and identify a critical problem associated with the deployment efficiency, i.e., prompt engineering.</p></li>
<li><p>To automate prompt engineering specifically for pre-trained vision-language models, we propose a simple approach based on continuous prompt learning and provide two implementations that can handle different recognition tasks.</p></li>
<li><p>We for the first time show that the proposed prompt learning-based approach outperforms both handcrafted prompts and the linear probe model in terms of downstream transfer learning performance and robustness under domain shifts for large visionlanguage models.</p></li>
</ol>
</section>
<section id="related-work" class="level1">
<h1>Related Work</h1>
<p>~~</p>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<p>As mentioned earlier, CoOp has organized two contexts. Before we introduce the unified context, we will show the prediction probability used by CLIP.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Prediction Probability of CLIP</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig8.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Itâ€™s organized like the formula shown above</p>
<section id="unified-context" class="level3">
<h3 class="anchored" data-anchor-id="unified-context">Unified Context</h3>
<p>A unified context means that all classes have the same context.</p>
<p>In this case, prompt t could be expressed as follows</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>where each <img src="https://latex.codecogs.com/png.latex?%7B%5BV%5D_m%20(m%20%5Cin%7B%20(1,%20...,%20M)%20%7D)%7D"> is a vector with the same dimension as word embeddings (i.e., 512 for CLIP), and is a hyperparameter specifying the number of context tokens.</p>
<p>By forwarding a prompt ğ’• to the text encoder g(â‹…), we can obtain a classification weight vector representing a visual concept (still from the [EOS] token position).</p>
<p>The prediction probability is computed as</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Other than placing the class token at the end of a sequence as in Equation (2), we can also put it in the middle like</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>which increases flexibility for learningâ€”the prompt is allowed to either fill the latter cells with supplementary descriptions or cut off the sentence earlier by using a termination signal such as full stop.</p>
</section>
<section id="class-specific-context" class="level3">
<h3 class="anchored" data-anchor-id="class-specific-context">Class-Specific Context</h3>
<p>Another option for CoOp is Class-Specific Context (CSC).</p>
<p>CSC uses an independent context for each class, as opposed to a unified context, which can be represented by the formula</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/fig7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="experiments-and-discussions" class="level1">
<h1>Experiments and Discussions</h1>
<p>~~</p>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>


</section>

 ]]></description>
  <category>VQA</category>
  <category>Multi-Modal</category>
  <guid>https://ui-seok.github.io/docs/paper/posts/2024-11-10-CoOp/index.html</guid>
  <pubDate>Sat, 09 Nov 2024 15:00:00 GMT</pubDate>
  <media:content url="https://picsum.photos/id/201/200/115" medium="image"/>
</item>
<item>
  <title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title>
  <dc:creator>Ui Seok</dc:creator>
  <link>https://ui-seok.github.io/docs/paper/posts/2024-10-13-MALMM/index.html</link>
  <description><![CDATA[ 



<p>Paper Link : <a href="https://arxiv.org/abs/2404.05726">MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</a></p>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ê³µìœ¼ë¡œ ë¹„ì „ ëª¨ë¸ì„ LLMì— í†µí•©í•˜ì—¬ ë¹„ì „-ì–¸ì–´ ê¸°ë°˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ìµœê·¼ ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.</p>
<p>í•˜ì§€ë§Œ ê¸°ì¡´ì˜ LLM ê¸°ë°˜ ëŒ€í˜• ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(ì˜ˆ: Video-LLaMA, VideoChat)ì€ ì§§ì€ ì˜ìƒ ì´í•´ì— í•œì •ëœ í”„ë ˆì„ ìˆ˜ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ì´ ì—°êµ¬ì—ì„œëŠ” ì£¼ë¡œ ì¥ì‹œê°„ì˜ ë™ì˜ìƒ ì´í•´ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ì„¤ê³„í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.</p>
<p>ê¸°ì¡´ ì—°êµ¬ë“¤ì²˜ëŸ¼ ë” ë§ì€ í”„ë ˆì„ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹  ì˜¨ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ë™ì˜ìƒì„ ì²˜ë¦¬í•˜ê³  ê³¼ê±° ë™ì˜ìƒ ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ ë±…í¬ì— ì €ì¥í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.</p>
<p>ì´ë¥¼ í†µí•´ ìš°ë¦¬ ëª¨ë¸ì€ LLMì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œì•½ì´ë‚˜ GPU ë©”ëª¨ë¦¬ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šê³ ë„ ì¥ê¸° ë¶„ì„ì„ ìœ„í•´ ê³¼ê±° ë¹„ë””ì˜¤ ì½˜í…ì¸ ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ë©”ëª¨ë¦¬ ë±…í¬ëŠ” ê¸°ì„±í’ˆ ë°©ì‹ìœ¼ë¡œ í˜„ì¬ì˜ ë©€í‹°ëª¨ë‹¬ LLMì— ì›í™œí•˜ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ê¸´ ë¹„ë””ì˜¤ ì´í•´, ë¹„ë””ì˜¤ ì§ˆë¬¸ ë‹µë³€, ë¹„ë””ì˜¤ ìº¡ì…˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ì´í•´ ì‘ì—…ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìœ¼ë©°, ìš°ë¦¬ ëª¨ë¸ì€ ì—¬ëŸ¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>~~</p>
</section>
<section id="related-work" class="level1">
<h1>Related Work</h1>
<p>~~</p>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¥ê¸° ë¹„ë””ì˜¤ ì´í•´ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì¦ê°• ëŒ€ê·œëª¨ ë‹¤ì¤‘ ëª¨ë“œ ëª¨ë¸ì¸ MA-LMMì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>
<p>ëŒ€ë¶€ë¶„ì˜ ë™ì˜ìƒ ì´í•´ ë°©ë²•[42, 31, 43, 44, 45, 46, 47, 48, 49]ì—ì„œì²˜ëŸ¼ ë” ë§ì€ í”„ë ˆì„ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹  ì˜¨ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ìë™ íšŒê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•˜ë©°, ì´ëŠ” MeMViT [41]ì—ì„œ ì œì‹œí•œ ì¥ê¸° ê¸°ì–µ ì„¤ê³„ë¥¼ í†µí•œ ì˜¨ë¼ì¸ ì²˜ë¦¬ ë°©ì‹ì—ì„œ ì˜ê°ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.</p>
<p>ê·¸ë¦¼ 2(a)ëŠ” MA-LMM í”„ë ˆì„ì›Œí¬ì˜ ê°œìš”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>
<p>ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì˜ ìœ ì‚¬í•œ ì‚¬ë¡€[7, 9, 8, 12]ë¥¼ ë”°ë¼ ì „ì²´ ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” (1) ê³ ì •ëœ ì‹œê° ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ(Sec. 3.1), (2) ì‹œê° ë° í…ìŠ¤íŠ¸ ì„ë² ë”© ê³µê°„ì„ ì •ë ¬í•˜ê¸° ìœ„í•´ í›ˆë ¨ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ë³€í™˜ê¸°(Q-Former)ë¥¼ ì‚¬ìš©í•œ ì¥ê¸° ì‹œê°„ ëª¨ë¸ë§(Sec. 3.2), (3) ê³ ì •ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë””ì½”ë”©(Sec. 3.3) ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</section>
<section id="experiments-and-discussions" class="level1">
<h1>Experiments and Discussions</h1>
<p>~~</p>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<p><a href="https://arxiv.org/abs/2301.12597">BLIPv2 (Q-Former)</a></p>


</section>

 ]]></description>
  <category>VQA</category>
  <category>Multi-Modal</category>
  <guid>https://ui-seok.github.io/docs/paper/posts/2024-10-13-MALMM/index.html</guid>
  <pubDate>Sat, 12 Oct 2024 15:00:00 GMT</pubDate>
  <media:content url="https://picsum.photos/id/201/200/110" medium="image"/>
</item>
<item>
  <title>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
  <dc:creator>Ui Seok</dc:creator>
  <link>https://ui-seok.github.io/docs/paper/posts/2024-09-22-BLIP2/index.html</link>
  <description><![CDATA[ 



<p>Paper Link : <a href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></p>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models.</p>
<p>BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages.</p>
<p>The first stage bootstraps vision-language representation learning from a frozen image encoder.</p>
<p>The second stage bootstraps vision-to-language generative learning from a frozen language model.</p>
<p>BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>~~</p>
</section>
<section id="related-work" class="level1">
<h1>Related Work</h1>
<p>~~</p>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<p>We propose BLIP-2, a new vision-language pre-training method that bootstraps from frozen pre-trained unimodal models.</p>
<p>In order to bridge the modality gap, we propose a Querying Transformer (Q-Former) pre-trained in two stages: (1) vision-language representation learning stage with a frozen image encoder and (2) vision-to-language generative learning stage with a frozen LLM.</p>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">3.1 Model Architecture</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-09-22-BLIP2/fig2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Model Architecture</figcaption>
</figure>
</div>
<p>We propose Q-Former as the trainable module to bridge the gap between a frozen image encoder and a frozen LLM.</p>
<p>It extracts a fixed number of output features from the image encoder, independent of input image resolution.</p>
<p>As shown in upper Figure, Q-Former consists of two transformer submodules that share the same self-attention layers: (1) an image transformer that interacts with the frozen image encoder for visual feature extraction, (2) a text transformer that can function as both a text encoder and a text decoder.</p>
<p>We create a set number of learnable query embeddings as input to the image transformer.</p>
<p>-&gt; Left of Q-Former box</p>
<p>The queries interact with each other through self-attention layers, and interact with frozen image features through cross-attention layers (inserted every other transformer block).</p>
<p>The queries can additionally interact with the text through the same self-attention layers.</p>
<p>Depending on the pre-training task, we apply different self-attention masks to control query-text interaction.</p>
<hr>
<ul>
<li><p>Q-Former is a simple transformer structure that is utilized to extract information from a frozen image encoder.</p></li>
<li><p>In the first step, it is trained to extract text-related information from the image, and in the second step, the extracted information is trained to be interpretable by the LLM. As a result, it is possible to leverage the zero-shot performance of the LLM for VL tasks while gaining the parameter efficiency of frozen large models.</p></li>
</ul>
<hr>
</section>
<section id="bootstrap-vision-language-representation-learning-from-a-frozen-image-encoder" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-vision-language-representation-learning-from-a-frozen-image-encoder">3.2 Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-09-22-BLIP2/fig3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Model Architecture</figcaption>
</figure>
</div>
<p>In the representation learning stage, we connect Q-Former to a frozen image encoder and perform pre-training using image-text pairs.</p>
<p>We aim to train the Q-Former such that the queries can learn to extract visual representation that is most informative of the text.</p>
<p>Inspired by <strong>BLIP</strong>, we jointly optimize three pre-training objectives that share the same input format and model parameters.</p>
<p><strong>Image-Text Contrastive Learning (ITC)</strong> learns to align image representation and text representation such that their mutual information is maximized.</p>
<p>It achieves so by contrasting the image-text similarity of a positive pair against those of negative pairs.</p>
<p>-&gt; To prevent this, we use the Uni-modal Self-Attention Mask because it would be cheating if images and text referenced each otherâ€™s information.</p>
<p><strong>Image-grounded Text Generation (ITG)</strong> loss trains the Q-Former to generate texts, given input images as the condition.</p>
<p>-&gt; Utilize a Multi-modal Causal Self-Attention Mask to prevent the query from referencing text information because it would be cheating for the query to preview text information. Also design the self-attention mask so that the text generation task can only reference text before the current point in time.</p>
<p><strong>Image-Text Matching (ITM)</strong> aims to learn fine-grained alignment between image and text representation.</p>
<p>It is a binary classification task where the model is asked to predict whether an image-text pair is positive (matched) or negative (unmatched).</p>
<p>-&gt; Since itâ€™s okay to look at all the information we have, we use a Bi-directional Self-Attention Mask.</p>
</section>
<section id="bootstrap-vision-to-language-generative-learning-from-a-frozen-llm" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-vision-to-language-generative-learning-from-a-frozen-llm">3.3 Bootstrap Vision-to-Language Generative Learning from a Frozen LLM</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-09-22-BLIP2/fig4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Second Architecture</figcaption>
</figure>
</div>
<p>In the generative pre-training stage, we connect Q-Former (with the frozen image encoder attached) to a frozen LLM to harvest the LLMâ€™s generative language capability.</p>
<p>As shown in upper Figure, we use a fully-connected (FC) layer to linearly project the output query embeddings Z into the same dimension as the text embedding of the LLM.</p>
<p>The projected query embeddings are then prepended to the input text embeddings.</p>
<p>They function as soft visual prompts that condition the LLM on visual representation extracted by the Q-Former.</p>
<p>Since the Q-Former has been pre-trained to extract language-informative visual representation, it effectively functions as an information bottleneck that feeds the most useful information to the LLM while removing irrelevant visual information.</p>
</section>
<section id="model-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="model-pre-training">3.4 Model Pre-training</h3>
<p>We use the same pre-training dataset as BLIP</p>
</section>
</section>
<section id="experiments-and-discussions" class="level1">
<h1>Experiments and Discussions</h1>
<p>~~</p>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<p><a href="https://arxiv.org/abs/2201.12086">BLIP</a></p>


</section>

 ]]></description>
  <category>VQA</category>
  <guid>https://ui-seok.github.io/docs/paper/posts/2024-09-22-BLIP2/index.html</guid>
  <pubDate>Sat, 21 Sep 2024 15:00:00 GMT</pubDate>
  <media:content url="https://picsum.photos/id/201/200/102" medium="image"/>
</item>
<item>
  <title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title>
  <dc:creator>Ui Seok</dc:creator>
  <link>https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/index.html</link>
  <description><![CDATA[ 



<p>Paper Link : <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.</p>
<p>However, most existing pre-trained models only excel in either <strong><code>understanding-based</code></strong> tasks or <strong><code>generation-based</code></strong> tasks.</p>
<p>Furthermore, performace improvement has been largely achieved by scaling up the dataset with <strong><code>noisy image-text pairs collected from the web</code></strong>.</p>
<p>BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones (called CapFilt).</p>
<p>Also, denmonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>~~</p>
</section>
<section id="related-work" class="level1">
<h1>Related Work</h1>
<p>~~</p>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<p>This section first introduces our new model architecture <strong><code>MED</code></strong> and its pre-training objectives, and then delineates <strong><code>CapFilt</code></strong> for dataset bootstrapping.</p>
<section id="model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture">3.1 Model Architecture</h3>
<p>We employ a visual transformer (ViT) as our image encoder, which devides an input image into patches and encodes them as a sequence of embeddings, with an additional [CLS] token to represent the global image feature.</p>
<p>-&gt; [CLS] token is used as a token to indicate the start of a sentence.</p>
<p>In order to pre-train a unified model with both understanding and generation capabilities, we propose <strong><code>multi-model mixture of encoder-decoder (MED)</code></strong>, a multi-task model which can operate in one of the three functionalities.</p>
<ol type="1">
<li><p>Unimodal encoder, which separately encodes image and text</p>
<img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig1.png" class="img-fluid" data-fig-align="left" alt="Unimodal Encoder">
<p>The text encoder is the same as BERT, where a [CLS] token is appended to the beginning of the text input to summarize the sentence.</p></li>
<li><p>Image-grounded text encoder</p>
<img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig2.png" class="img-fluid" data-fig-align="left" alt="Image-grounded Text Encoder">
<p>This injects visual information by inserting one additional cross-attention (CA) layer between the self-attention (SA) layer and the feed forward network (FFN) for each transformer block of the text encoder.</p>
<p>A task-specific [Encode] token is appended to the text, and the output embedding of [Encode] is used as the multimodal representation of the image-text pair.</p></li>
<li><p>Image-grounded text decoder</p>
<img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig3.png" class="img-fluid" data-fig-align="left" alt="Image-gronded Text Decoder">
<p>This replaces the bi-directional self-attention (Bi Self-Att block, in the image-grounded text encoder) with casual self-attention layers.</p>
<p>A [Decode] token is used to signal the beginning of a sequence, and an end-of-sequence token is used to signal its end.</p></li>
</ol>
</section>
<section id="pre-training-objectives" class="level3">
<h3 class="anchored" data-anchor-id="pre-training-objectives">3.2 Pre-training Objectives</h3>
<p>We jointly optimize three objectives during pre-training, with two understanding-based objectives and one generation-based objecttive.</p>
<p>Each image-text pair only requires one forward pass through the computational-heavier visual transformer, and three forward passes through the text transformer, where different functionalities are activated to compute the three losses as delineated below.</p>
<ol type="1">
<li><p>Image-Text Contrastive Loss (ITC)</p>
<p>This activates the unimodal encoder.</p>
<p>It aims to align the feature space of the visual transformer and the text transformer by encouraging positive image-text pairs to have similar representations in contrast to the negative pairs.</p>
<p>-&gt; Contrastive learning is a good way to think of it.</p>
<img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig4.png" class="img-fluid" data-fig-align="left" alt="Contrastive Leraning">
<p>We follow the ITC loss by <em>Align before fuse: Vision and language representation learning with momentum distillation</em></p>
<img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig5.png" class="img-fluid" data-fig-align="left" alt="ITC Loss"></li>
<li><p>Image-Text Matching Loss (ITM)</p>
<p>This activates the image-grounded text encoder.</p>
<p>It aims to learn image-text multimodal representation the captures the fine-grained alignment between vision and language.</p>
<p>ITM is a bianry classification task, where the model uses an ITM head (a linear layer) to predict whether an image-text pair is positive (matched) or negative (unmatched) given their multimodal feature.</p>
<p>-&gt; This is the loss of learning to predict whether the text and image are a good match after cross-attention with the text-embedding feauture and image embedding feature.</p></li>
<li><p>Language Modeling Loss (LM)</p>
<p>This is activates the image-grounded text decoder.</p>
<p>It aims to generate textual descriptions given an image.</p>
<p>It optimizes a cross entropy loss which trains the model to maximize the likelihood of the text in an autoregressive manner.</p>
<p>-&gt; Similar to ITM, the features from the image encoder are received through cross attention, and Language Modeling Loss is used to train the decoder to generate captions for the image.</p></li>
</ol>
</section>
<section id="capfilt" class="level3">
<h3 class="anchored" data-anchor-id="capfilt">3.3 CapFilt</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/fig6.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Learning framework of BLIP</figcaption>
</figure>
</div>
<p>Due to the prohibitive annotation cost, there exist a limited number of high-quality human-annotated image-text pairs <img src="https://latex.codecogs.com/png.latex?%7B(I_h,%20T_h)%7D">.</p>
<p>Recent work utilizes a much larger number of image and alt-text pairs <img src="https://latex.codecogs.com/png.latex?%7B(I_w,%20T_w)%7D"> that are automatically collected from the web.</p>
<p>However, the alt-texts often do not accurately describe the visual content of the images, making them a noisy signal that is suboptimal for learning vision-language alignment.</p>
<p>We propose Captioning and Filtering (CapFilt), a new method to improve the quality of the text corpus.</p>
<p>Both the captioner and the filter are initialized from the same pre-trained MED model, and finetuned individually on the COCO dataset.</p>
<p>Specifically, the captioner is an <strong><code>image-grounded text decoder</code></strong>.</p>
<p>It is finetuned with the <strong>LM</strong> objective to decode texts given images.</p>
<p>Given the web images <img src="https://latex.codecogs.com/png.latex?%7BI_w%7D">, the captioner generates synthetic captions <img src="https://latex.codecogs.com/png.latex?%7BT_s%7D"> with one caption per image.</p>
<p>The filter is an <strong><code>image-grounded text encoder</code></strong>.</p>
<p>It is finetuned with the <strong>ITC</strong> and <strong>ITM</strong> objectives to learn whether a text matches an image.</p>
<p>The filter removes noisy texts in both the original web texts <img src="https://latex.codecogs.com/png.latex?%7BT_w%7D"> and the synthetic texts <img src="https://latex.codecogs.com/png.latex?%7BT_s%7D">, where a text is considered to be noisy if the ITM head predicts it as unmatched to the image.</p>
<p>Finally, we combine the filtered image-text pairs with the human-annotated pairs to form a new dataset, which we use to pre-train a new model.</p>
<ul>
<li><p>Captioner (Image-grounded Text Encoder)</p>
<p>Given the human-labeled <img src="https://latex.codecogs.com/png.latex?%7B(I_h,%20T_h)%7D"> dataset and trained with LM loss, Captioner generates text for images collected from the web.</p></li>
<li><p>Filter (Image-grounded Text Encoder)</p>
<p>A dataset of <img src="https://latex.codecogs.com/png.latex?%7B(I_h,%20T_h)%7D"> pairs labeled by humans and a filter trained on ITC and ITM loss is used to filter the <em>{Image, Text}</em> pairs collected from the web <img src="https://latex.codecogs.com/png.latex?%7BT_w%7D"> and those generated <img src="https://latex.codecogs.com/png.latex?%7BT_s%7D"> by Captioner.</p>
<p>In this case, it serves to refine data that may be noisy by removing cases where <em>{Image, Text}</em> pairs are incorrectly paired.</p></li>
</ul>
</section>
</section>
<section id="experiments-and-discussions" class="level1">
<h1>Experiments and Discussions</h1>
<p>~~</p>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<p><a href="https://arxiv.org/abs/2107.07651">Align before fuse: Vision and language representation learning with momentum distillation</a></p>
<p><a href="https://arxiv.org/abs/2002.05709">Contrastive Learning</a></p>


</section>

 ]]></description>
  <category>VQA</category>
  <guid>https://ui-seok.github.io/docs/paper/posts/2024-09-15-BLIP/index.html</guid>
  <pubDate>Sat, 14 Sep 2024 15:00:00 GMT</pubDate>
  <media:content url="https://picsum.photos/id/201/200/101" medium="image"/>
</item>
</channel>
</rss>
